from unittest.mock import MagicMock
import inspect
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, col
import importlib
import sys
import os


globalSession = None


class FS(object):
    def __init__(self):
        self.help = MagicMock()
        self.cp = MagicMock(return_value=True)
        self.head = MagicMock(return_value="")
        self.ls = MagicMock(return_value=[])
        self.mkdirs = MagicMock(return_value=True)
        self.mv = MagicMock(return_value=True)
        self.put = MagicMock(return_value=True)
        self.rm = MagicMock(return_value=True)
        self.mount = MagicMock(return_value=True)
        self.mounts = MagicMock(return_value=[])
        self.refreshMounts = MagicMock(return_value=True)
        self.unmount = MagicMock(return_value=True)


class WorkflowInterrupted(Exception):
    pass


class Workflow(object):
    def __init__(self):
        self.help = MagicMock()
        self.run = MagicMock(return_value="")

    def exit(self, value: str):
        raise WorkflowInterrupted


class Widgets(object):
    def __init__(self):
        self.help = MagicMock()
        self.combobox = MagicMock()
        self.dropdown = MagicMock()
        self.get = MagicMock(return_value="")
        self.getArgument = MagicMock(return_value="")
        self.multiselect = MagicMock()
        self.remove = MagicMock()
        self.removeAll = MagicMock()
        self.text = MagicMock()


class Secrets(object):
    def __init__(self):
        self.help = MagicMock()
        self.get = MagicMock(return_value="")
        self.getBytes = MagicMock(return_value=bytearray())
        self.list = MagicMock(return_value=[])
        self.listScopes = MagicMock(return_value=[])


class Library(object):
    def __init__(self):
        self.help = MagicMock()
        self.install = MagicMock(return_value=True)
        self.installPyPI = MagicMock(return_value=True)
        self.list = MagicMock(return_value=[])
        self.restartPython = MagicMock()


class DbUtils(object):
    def __init__(self):
        self.fs = FS()
        self.notebook = Workflow()
        self.widgets = Widgets()
        self.secrets = Secrets()
        self.library = Library()


class Session():
    def __init__(self):
        self.display = MagicMock()
        self.displayHTML = MagicMock()
        self.dbutils = DbUtils()
        self.spark = (SparkSession.builder
                      .master("local")
                      .appName("test-pyspark")
                      #add delta lake support
                      .config("spark.jars.packages", "io.delta:delta-core_2.12:0.7.0")
                      .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
                      .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
                      #set default SerDe to parquet
                      .config("hive.default.fileformat","parquet")
                      .enableHiveSupport()
                      .getOrCreate())

    def run_notebook(self, dir, script):
        """
        Run a script such as a Databricks notebook
        """
        try:
            with add_path(os.path.abspath(dir)):
                if script not in sys.modules:
                    # Import will run the script only the first time
                    importlib.import_module(script)
                else:
                    # If script was already imported, reload it to rerun it
                    importlib.reload(sys.modules[script])
        except WorkflowInterrupted:
            pass

    def get_show_string(self, df, n=20, truncate=True, vertical=False)->str:
        """
        Returns the output of the `.show()` function as a string
        """
        if isinstance(truncate, bool) and truncate:
            return(df._jdf.showString(n, 20, vertical))
        else:
            return(df._jdf.showString(n, int(truncate), vertical))

    def assert_queries_are_equal(self, actual_query, expected_query):
        """
        Compares the results of two queries for equality

        The resultsets generated by the queries must have the same schema

        The detailed table comparison output is only shown in the event of a failure
        """
        result_df = self.spark.sql(f"""
            WITH actual
            AS
            ({actual_query})
            ,expected
            AS
            ({expected_query})
            ,matched
            AS
            (SELECT * FROM actual INTERSECT SELECT * FROM expected)
            ,missing
            AS
            (SELECT * FROM actual EXCEPT SELECT * FROM expected)
            ,extra
            AS
            (SELECT * FROM expected EXCEPT SELECT * FROM actual)
            SELECT '=' AS m, * FROM matched
            UNION ALL
            SELECT '>' AS m, * FROM missing
            UNION ALL
            SELECT '<' AS m, * FROM extra  
        """
        )
        
        failure_count = result_df.where(col("m").isin({">", "<"})).count()
        if failure_count > 0:
            msg = self.get_show_string(result_df, n=result_df.count(), truncate=False)
            assert failure_count == 0, f"the result sets did not match:\n{msg}"

    def assert_query_returns_no_rows(self, actual_query):
        """
        Asserts that a query returns an empty result set
        """
        result_df = self.spark.sql(actual_query)
        
        record_count = result_df.count()
        if record_count > 0:
            msg = self.get_show_string(result_df, n=result_df.count(), truncate=False)
            assert record_count == 0, f"the result set was not empty:\n{msg}"


def inject_variables():
    """
    Inject (real or mocked) variables in notebook scope, as Databricks does.
    """
    stack = inspect.stack()
    fvar = stack[1].frame.f_locals
    fvar['display'] = globalSession.display
    fvar['displayHTML'] = globalSession.displayHTML
    fvar['dbutils'] = globalSession.dbutils
    fvar['spark'] = globalSession.spark
    fvar['sc'] = globalSession.spark.sparkContext
    fvar['sqlContext'] = globalSession.spark
    fvar['table'] = globalSession.spark.table
    fvar['sql'] = globalSession.spark.sql
    fvar['udf'] = udf


class SessionAlreadyExistsException(Exception):
    pass


class session():
    """
    Context manager to override mocks within test scope
    """

    def __enter__(self):
        global globalSession
        if globalSession:
            raise SessionAlreadyExistsException("A session already exists")
        globalSession = Session()
        return globalSession

    def __exit__(self, exc_type, exc_value, traceback):
        global globalSession
        globalSession = None


class add_path():
    """
    Context manager to temporarily add an entry to sys.path
    """

    def __init__(self, path):
        self.path = path

    def __enter__(self):
        sys.path.insert(0, self.path)

    def __exit__(self, exc_type, exc_value, traceback):
        try:
            sys.path.remove(self.path)
        except ValueError:
            pass
